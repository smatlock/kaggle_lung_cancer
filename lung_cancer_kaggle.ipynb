{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b1bf5c-5ec8-40e5-93c6-5577be92cf41",
   "metadata": {},
   "source": [
    "# Lung Cancer Image Classification\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "### Lung Cancer Image Dataset: A Comprehensive Collection\n",
    "\n",
    "Explore the intricacies of lung cancer with our curated dataset, consisting of high-resolution CT scan images. This dataset is designed to aid researchers, clinicians, and machine learning/Deep learning enthusiasts in studying the diverse manifestations of lung cancer.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "#### CT Scan Images:\n",
    "Our dataset comprises CT scan images, providing detailed insights into lung cancer morphology. Each image is a visual representation of the complex nature of lung tumors.\n",
    "\n",
    "#### Split for Comprehensive Analysis:\n",
    "- **Training Set (613 Images)**: A robust training set containing 613 images meticulously labeled into four distinct classes, allowing for in-depth model training and understanding.\n",
    "- **Testing Set (315 Images)**: Evaluate the model's performance on a diverse range of 315 images, each belonging to one of the four well-defined lung cancer classes.\n",
    "- **Validation Set (72 Images)**: A curated validation set of 72 images, essential for fine-tuning models and ensuring generalizability.\n",
    "\n",
    "### Classes:\n",
    "- **Class 1**: Adenocarcinoma\n",
    "- **Class 2**: Large Cell Carcinoma\n",
    "- **Class 3**: Normal\n",
    "- **Class 4**: Squamous Cell Carcinoma\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/kabil007/lungcancer4types-imagedataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075de9e3-ad3d-472b-97db-cd81b6c879a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import cv2 \n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization,Dense, MaxPool2D, MaxPooling2D, Flatten,GlobalMaxPooling2D, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, Sequential, Model, load_model\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152, VGG16, VGG19, EfficientNetB0\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22738e09-9d5b-42f6-8348-07b81052fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "input_size = (224,224) #  Note: pre-trained models were trained on images of this size\n",
    "batch_size=32\n",
    "\n",
    "# Import test, train, and validation data\n",
    "test_data = ImageDataGenerator().flow_from_directory(\n",
    "    \"./archive/Data/test\",\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size,\n",
    "    target_size = input_size,\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "\n",
    "class_names = list(test_data.class_indices.keys())\n",
    "\n",
    "train_data = ImageDataGenerator().flow_from_directory(\n",
    "    './archive/Data/train',\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    target_size = input_size,\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "\n",
    "valid_data = ImageDataGenerator().flow_from_directory(\n",
    "    \"./archive/Data/valid\",\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size,\n",
    "    target_size = input_size,\n",
    "    class_mode = \"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6880082-f716-4d43-89b3-395b300afddb",
   "metadata": {},
   "source": [
    "## Let's take a look at images from each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6d9dc-5855-46ea-b768-01b26c7c769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class labels\n",
    "class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "# Function to load four images from each class\n",
    "def load_images_per_class(data_gen, class_labels, num_images=4):\n",
    "    images = {label: [] for label in class_labels}\n",
    "    while any(len(images[label]) < num_images for label in class_labels):\n",
    "        img_batch, label_batch = next(data_gen)\n",
    "        for img, label in zip(img_batch, label_batch):\n",
    "            class_idx = np.argmax(label)\n",
    "            class_label = class_labels[class_idx]\n",
    "            if len(images[class_label]) < num_images:\n",
    "                images[class_label].append(img)\n",
    "    return images\n",
    "\n",
    "# Load four images per class\n",
    "images_dict = load_images_per_class(test_data, class_labels, num_images=4)\n",
    "\n",
    "# Create subplots\n",
    "num_classes = len(class_labels)\n",
    "fig = make_subplots(rows=num_classes, cols=4, subplot_titles=[f\"{label} {i+1}\" for label in class_labels for i in range(4)])\n",
    "\n",
    "# Add images to subplots\n",
    "for class_idx, class_label in enumerate(class_labels):\n",
    "    for img_idx, img in enumerate(images_dict[class_label]):\n",
    "        fig.add_trace(\n",
    "            go.Image(z=img.astype(np.uint8)),\n",
    "            row=class_idx+1, col=img_idx+1\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=300*num_classes, width=1200, title_text=\"Sample Images from Each Class\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451f880-cc0d-4f04-a0b0-09ec9630cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy for each epoch and show where the highest accuracy & lowest loss are\n",
    "def plot_accuracy(history):\n",
    "\n",
    "    # Access the history data\n",
    "    history_dict = history.history\n",
    "    \n",
    "    # Extract metrics\n",
    "    accuracy = history_dict['accuracy']\n",
    "    val_accuracy = history_dict['val_accuracy']\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "    epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "    # Find the best validation accuracy and corresponding epoch\n",
    "    best_val_acc = max(val_accuracy)\n",
    "    best_val_acc_epoch = val_accuracy.index(best_val_acc) + 1\n",
    "\n",
    "    # Find the lowest validation loss and corresponding epoch\n",
    "    lowest_val_loss = min(val_loss)\n",
    "    lowest_val_loss_epoch = val_loss.index(lowest_val_loss) + 1\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "    plt.scatter(best_val_acc_epoch, best_val_acc, color='red', label=f'Best Val Accuracy (Epoch {best_val_acc_epoch})')\n",
    "    plt.text(best_val_acc_epoch, best_val_acc, f'{best_val_acc:.2f}', color='red', ha='right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.scatter(lowest_val_loss_epoch, lowest_val_loss, color='red', label=f'Lowest Val Loss (Epoch {lowest_val_loss_epoch})')\n",
    "    plt.text(lowest_val_loss_epoch, lowest_val_loss, f'{lowest_val_loss:.2f}', color='red', ha='right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3ee25-48dc-492d-b190-14d9a2081b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(test_data.class_indices.keys())\n",
    "\n",
    "def plot_confusion_matrix_and_report(model, test_data, class_names, checkpoint_path):\n",
    "\n",
    "    # Load the model weights from the checkpoint file\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    test_loss, test_accuracy = model.evaluate(test_data)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(test_data)\n",
    "    y_pred_classes = y_pred.argmax(axis=-1)\n",
    "    y_true = test_data.classes\n",
    "\n",
    "    # Print classification report\n",
    "    report = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    # Rotate the x-axis labels to 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8e5c1-9c2f-41e3-8a8c-eec07c078887",
   "metadata": {},
   "source": [
    "# ResNet50, 101, 152\n",
    "Let's try the different ResNet models. These models increase in complexity. We will use the evaluations of these models to choose additional pre-trained models to check our image sets with. I.e., if ResNet50 outperforms ResNet152, ResNet152 may be too complex resulting in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ada88-07b3-4d07-a2d0-e2ebd2737e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the same learning rate for these models\n",
    "learning_rate = 0.0001  # Note: we will attempt to finetune the learning rate later in the notebook\n",
    "\n",
    "# Set early stopping and checkpoints\n",
    "monitor=\"val_loss\"\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    patience=10,\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "# The checkpoints help in case our computer crashes or our compiling is interupted, but we also want to use the model from the epoch that performed the best.\n",
    "checkpoint_resnet50 = ModelCheckpoint(\n",
    "    'resnet50_best.weights.h5',\n",
    "    monitor=monitor,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "checkpoint_resnet101 = ModelCheckpoint(\n",
    "    'resnet101_best.weights.h5',\n",
    "    monitor=monitor,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "checkpoint_resnet152 = ModelCheckpoint(\n",
    "    'resnet152_best.weights.h5',\n",
    "    monitor=monitor,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0eda2-9712-4201-8073-6af29a5ab80d",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30683a59-ded9-41f5-b972-be503556d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = ResNet50( include_top=False, input_shape=(224, 224, 3))\n",
    "resnet50_model.trainable = False\n",
    "resnet50_model = Sequential ([\n",
    "    resnet50_model,\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "resnet50_model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_resnet50 = resnet50_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data,\n",
    "    epochs = 100, \n",
    "    callbacks=[early_stop, checkpoint_resnet50],\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d821d5-09c9-4c46-bc5d-f3b341457de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history_resnet50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f96757-8228-4128-bb8d-574437cbfd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_acc, resnet50_loss = plot_confusion_matrix_and_report(resnet50_model, test_data, class_names, './resnet50_best.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55175e92-c9c8-421c-be9c-7038bb655bb1",
   "metadata": {},
   "source": [
    "## ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675f633-5f86-461d-98b1-66f15a41e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101_model = ResNet101( include_top=False, input_shape=(224, 224, 3))\n",
    "resnet101_model.trainable = False\n",
    "resnet101_model = Sequential ([\n",
    "    resnet101_model,\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "resnet101_model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_resnet101 = resnet101_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data,\n",
    "    epochs = 100, \n",
    "    callbacks=[early_stop, checkpoint_resnet101],\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d45a18-4545-46b9-9ad5-dc7036b9a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history_resnet101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e3ce5-af61-43bc-bf9a-93e06feaa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101_acc, resnet101_loss = plot_confusion_matrix_and_report(resnet101_model, test_data, class_names, './resnet101_best.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebbbf6-6a5f-42ba-8bb8-b05019669ffe",
   "metadata": {},
   "source": [
    "## ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13dd430-196d-4701-b9ea-d2862fd6d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152_model = ResNet152( include_top=False, input_shape=(224, 224, 3))\n",
    "resnet152_model.trainable = False\n",
    "resnet152_model = Sequential ([\n",
    "    resnet152_model,\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "resnet152_model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_resnet152 = resnet152_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data,\n",
    "    epochs = 100, \n",
    "    callbacks=[early_stop, checkpoint_resnet152],\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6483690-9e59-4861-86c8-b7362a93b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history_resnet152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ded768-3e38-40f3-acb3-47ecab178bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152_acc, resnet152_loss = plot_confusion_matrix_and_report(resnet152_model, test_data, class_names, './resnet152_best.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abde05-83d6-44aa-9bf7-82cd4dd11558",
   "metadata": {},
   "source": [
    "## Let's see which ResNet model performed the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a352b85-4241-41ec-8c33-bb6967c61128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(model_names, accuracies, losses):\n",
    "\n",
    "    # Create a DataFrame from the lists for easy plotting\n",
    "    data = {\n",
    "        'Model': model_names,\n",
    "        'Accuracy': accuracies,\n",
    "        'Loss': losses\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Next, create a bar plot for accuracies and losses\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Bar plot for accuracies\n",
    "    sns.barplot(x='Model', y='Accuracy', data=df, ax=ax1, palette='viridis', alpha=0.7)\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Test Data Performance Comparison per Model')\n",
    "\n",
    "    # Add accuracy values on top of the bars (note, higher accuracy is better)\n",
    "    for p in ax1.patches:\n",
    "        ax1.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha='center', va='center', \n",
    "                     xytext=(0, 9), \n",
    "                     textcoords='offset points',\n",
    "                     color='black', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Create a secondary y-axis for losses\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(x='Model', y='Loss', data=df, ax=ax2, color='red', marker='o', linewidth=2.5)\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_ylim(0, max(losses) * 1.2)\n",
    "\n",
    "    # Add loss values above the points on the line plot (note, lower loss is better)\n",
    "    for line in ax2.lines:\n",
    "        for x, y in zip(line.get_xdata(), line.get_ydata()):\n",
    "            ax2.annotate(f'{y:.2f}', \n",
    "                         (x, y), \n",
    "                         ha='center', va='bottom', \n",
    "                         xytext=(0, 5), \n",
    "                         textcoords='offset points',\n",
    "                         color='red', fontsize=10, fontweight='bold')\n",
    "        \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "resnet_models = [\"ResNet50\",\"ResNet101\",\"ResNet152\"]\n",
    "resnet_accs = [resnet50_acc, resnet101_acc, resnet152_acc]\n",
    "resnet_loss = [resnet50_loss, resnet101_loss, resnet152_loss]\n",
    "\n",
    "plot_comparison(resnet_models, resnet_accs, resnet_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f7dce-118e-411c-9119-07652c7cec35",
   "metadata": {},
   "source": [
    "## Looks like our ResNet101 Model performed the best out of the ResNet Models!\n",
    "Let's try VGG16, VGG19,  and EfficientNetB0. We will take the best of the six models and see if we can hypertune some of their parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b18e10-5d9a-4329-9d9c-2a9ad2179378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the same learning rate for these models\n",
    "learning_rate = 0.0001 \n",
    "\n",
    "# Set early stopping and checkpoints\n",
    "monitor=\"val_loss\"\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    patience=10,\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "# The checkpoints help in case our computer crashes or our compiling is interupted, but we also want to use the model from the epoch that performed the best.\n",
    "checkpoint_VGG16 = ModelCheckpoint(\n",
    "    'VGG16_best.weights.h5',\n",
    "    monitor=monitor,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "checkpoint_VGG19 = ModelCheckpoint(\n",
    "    'VGG19_best.weights.h5',\n",
    "    monitor=monitor,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "checkpoint_EfficientNetB0 = ModelCheckpoint(\n",
    "    'EfficientNetB0_best.weights.h5',\n",
    "    monitor=monitor,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193c804-49ea-4263-bd69-c5c2129503cb",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72103a-cd18-4db6-bbbe-4ace7b36ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model = VGG16( include_top=False, input_shape=(224, 224, 3))\n",
    "VGG16_model.trainable = False\n",
    "VGG16_model = Sequential([\n",
    "    VGG16_model,\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "VGG16_model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_VGG16 = VGG16_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data,\n",
    "    epochs = 100, \n",
    "    callbacks=[early_stop, checkpoint_VGG16],\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125458e-ff55-4975-ad7c-94b25a311d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history_VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a616e-927f-422c-8c6c-4ae5482dd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_acc, VGG16_loss = plot_confusion_matrix_and_report(VGG16_model, test_data, class_names, './VGG16_best.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30de273-23ce-4f09-a10f-890e50618b6d",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b9a63-354e-4ef3-963c-bfba455b8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_model = VGG19( include_top=False, input_shape=(224, 224, 3))\n",
    "VGG19_model.trainable = False\n",
    "VGG19_model = Sequential([\n",
    "    VGG19_model,\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "VGG19_model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_VGG19 = VGG19_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data,\n",
    "    epochs = 100, \n",
    "    callbacks=[early_stop, checkpoint_VGG19],\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76cda0-a154-4d18-8b5d-cdc2ae6d8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history_VGG19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fe906-a6eb-4f75-8f2f-f89accb6cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_acc, VGG19_loss = plot_confusion_matrix_and_report(VGG19_model, test_data, class_names, './VGG19_best.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5650c9-c79c-45f6-851d-21f247c6f9c9",
   "metadata": {},
   "source": [
    "## EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0ce40-9cbb-46db-bf39-ad00e23e2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNetB0_model = EfficientNetB0(include_top=False, input_shape=(224, 224, 3))\n",
    "EfficientNetB0_model.trainable = False\n",
    "EfficientNetB0_model = Sequential([\n",
    "    EfficientNetB0_model,\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "EfficientNetB0_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_EfficientNetB0 = EfficientNetB0_model.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop, checkpoint_EfficientNetB0],\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bcaac7-86d4-4879-8196-984396a48fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history_EfficientNetB0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd00a90-5e8d-4a96-9350-6f22b7ea80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNetB0_acc, EfficientNetB0_loss = plot_confusion_matrix_and_report(EfficientNetB0_model, test_data, class_names, './EfficientNetB0_best.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679ea02-a296-46d4-91e6-60fffe1b0546",
   "metadata": {},
   "source": [
    "## Let's evaluate our VGGs and EfficientNetB0 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fe4df-c9e3-4909-9504-359451fd8f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_models = [\"VGG16\",\"VGG19\",\"EfficientNetB0\"]\n",
    "bonus_accs = [VGG16_acc, VGG19_acc, EfficientNetB0_acc]\n",
    "bonus_loss = [VGG16_loss, VGG19_loss, EfficientNetB0_loss]\n",
    "\n",
    "plot_comparison(bonus_models, bonus_accs, bonus_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878969c-21b9-4203-9688-31f7b30a2d17",
   "metadata": {},
   "source": [
    "## Initial Conclusion\n",
    "### VGG16 and VGG19 perform the best on this lung cancer image data set! 87.62% accuracy is pretty good, but let's see if we can get it up to 90% while either maintaining our 0.41 loss or lowering it as well.\n",
    "Let's try:\n",
    "1. Data augmentation\n",
    "2. Learning rate adjustment\n",
    "3. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552416c1-0856-4bec-bab5-8ef68ca331d3",
   "metadata": {},
   "source": [
    "## Data augmentation for VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e7061-a17f-44c2-aa12-01ec944efb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented checkpoint to compare to the original VGG16\n",
    "checkpoint_augmented_VGG16 = ModelCheckpoint(\n",
    "    'augmented_VGG16_best.weights.h5',\n",
    "    monitor=monitor,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "# Set up data augmentation for the training data\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    './archive/Data/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    './archive/Data/valid',\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size,\n",
    "    target_size = input_size,\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "augmented_VGG16_history = VGG16_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop, checkpoint_augmented_VGG16],\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd1c17-73fa-413b-b145-3e5e049e0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy for augmented VGG16\n",
    "plot_accuracy(augmented_VGG16_history)\n",
    "\n",
    "# Evaluate augmented VGG16 and plot confusion matrix\n",
    "augment_VGG16_acc, augment_VGG16_loss = plot_confusion_matrix_and_report(VGG16_model, test_data, class_names, './augmented_VGG16_best.weights.h5')\n",
    "\n",
    "# Compare models\n",
    "vgg_models = [\"VGG16\", \"VGG16_aug\"]\n",
    "vgg_accs = [VGG16_acc, augment_VGG16_acc]\n",
    "vgg_loss = [VGG16_loss, augment_VGG16_loss]\n",
    "\n",
    "plot_comparison(vgg_models, vgg_accs, vgg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b356d-b015-4a17-8dbd-032340bae70a",
   "metadata": {},
   "source": [
    "## Adjust the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78b1bc-8b60-4397-b5b9-af10dbae4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_learning_rates(lr):\n",
    "    checkpoint_path  = './VGG16_LR_' + str(lr) + '_best.weights.h5'\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    VGG16_reg_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    "    VGG16_reg_model.trainable = False\n",
    "    \n",
    "    VGG16_reg_model = Sequential([\n",
    "        VGG16_reg_model,\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    VGG16_reg_model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "                                 \n",
    "    history = VGG16_reg_model.fit(\n",
    "        train_generator,\n",
    "        validation_data=valid_generator,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stop, model_checkpoint],\n",
    "        batch_size=batch_size,\n",
    "        verbose=2\n",
    "    )\n",
    "    return VGG16_reg_model, history\n",
    "\n",
    "# Train the models with different learning rates\n",
    "\n",
    "learning_rates = [0.00001, 0.001, 0.01] # We already have a model of 0.0001\n",
    "lr_losses = [VGG16_loss]\n",
    "lr_accuracies = [VGG16_acc]\n",
    "lr_titles = [\"VGG16_LR_0.0001\"]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    model, history = model_learning_rates(lr)\n",
    "    plot_accuracy(history)\n",
    "    acc, loss = plot_confusion_matrix_and_report(model, test_data, class_names, './VGG16_LR_' + str(lr) + '_best.weights.h5')\n",
    "    lr_losses.append(loss)\n",
    "    lr_accuracies.append(acc)\n",
    "    lr_titles.append(\"VGG16_LR_\"+str(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad844c8-8c32-4e98-9567-82bd10386280",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(lr_titles, lr_accuracies, lr_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d906a2-7206-4bb3-9d0a-05326321c037",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Let's try adding some L2 (ridge) regularization. Regularization can be helpful when you have a complex model with lots of features like we have with our VGG16 model. L2 helps reduce overfitting by penalizing large weights.\n",
    "\n",
    "Let's try a couple different lambdas:\n",
    "\n",
    "- L2 Regularization (λ = 0.001): Provides a moderate regularization effect.\n",
    "- L2 Regularization (λ < 0.001): Weaker regularization, allowing more flexibility.\n",
    "- L2 Regularization (λ > 0.001): Stronger regularization, reducing the model’s capacity to fit noise in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde72263-aca8-470c-9cfe-b8a9f4c9819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_lambdas(lamb):\n",
    "    checkpoint_path  = './VGG16_lamda_' + str(lamb) + '_best.weights.h5'\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    VGG16_reg_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    "    VGG16_reg_model.trainable = False\n",
    "    \n",
    "    VGG16_reg_model = Sequential([\n",
    "        VGG16_reg_model,\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(lamb)),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # back to our 0.0001 LR since it performed the best\n",
    "    VGG16_reg_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "                                 \n",
    "    history = VGG16_reg_model.fit(\n",
    "        train_generator,\n",
    "        validation_data=valid_generator,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stop, model_checkpoint],\n",
    "        batch_size=batch_size,\n",
    "        verbose=2\n",
    "    )\n",
    "    return VGG16_reg_model, history\n",
    "\n",
    "# Train the models with different lambdas\n",
    "l2_lambdas = [0.0001, 0.001, 0.01]\n",
    "lambda_losses = []\n",
    "lambda_accuracies = []\n",
    "lambda_titles = []\n",
    "\n",
    "for lam in l2_lambdas:\n",
    "    print(f\"Training with l2 reg: {lam}\")\n",
    "    model, history = model_with_lambdas(lam)\n",
    "    plot_accuracy(history)\n",
    "    acc, loss = plot_confusion_matrix_and_report(model, test_data, class_names, './VGG16_lamda_' + str(lam) + '_best.weights.h5')\n",
    "    lambda_losses.append(loss)\n",
    "    lambda_accuracies.append(acc)\n",
    "    lambda_titles.append(\"VGG16_LR_\"+str(lam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef2675-afca-47fe-8e54-130008ff83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison([\"VGG16_LR_\"+str(0.0001),\"VGG16_LR_\"+str(0.001),\"VGG16_LR_\"+str(0.01)], lambda_accuracies, lambda_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596cb7e-f191-4867-a7e5-0047ba8b3f07",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The hyperparameter investigation led to worse performance/overfitting. Therefore...\n",
    "## Out of the box VGG16 is the best model to use for this lung cancer set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57ef3f-3df4-4516-a6ac-93ac9ac98790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
